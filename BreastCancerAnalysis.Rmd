---
title: 'Group Assignment 1: Breast Cancer Analysis using KNN'
author: "Aylin Kosar, Surya Aenuganti Ushasri, Salma Olmai, Nicolas Romero, Viraj Prasad Sapre"
date: "October 16, 2018"
output:
  pdf_document: default
  html_document: default
---

### **Upload Data**
*Surya*: 
```{r}
#setwd("~/Fall 2018 R Data Files") 
#Load the data from a csv file
cancer = read.csv("wisc_bc_data.csv", na.strings = NULL)
```


### **Check Data**
*Surya*: 
```{r}
#Check the structure of the data
str(cancer) 
```

### **Organize Data**
*Surya*: 
```{r}
table(cancer$diagnosis)
```


```{r}
#Check whether there is any missing data
sum(is.na(cancer$diagnosis))
```


```{r}
#Clearly this should be a factor hence converting it to a factor and labeling the levels to benign or malignant
cancer$diagnosis <- factor(cancer$diagnosis, levels = c("B", "M"), labels = c("benign","malignant"))
levels(cancer$diagnosis)
```
 
*Surya*: Since we need to measure distances to classify them according to knn, we need the variables to have numerical values on a same scale. So we normalize the variables. Here we are creating a function so that we can apply the normalization to all columns using this single function.
```{r}
normalize = function(x) {
    y = (x - min(x))/(max(x) - min(x))
    y
}

```


```{r}
#Applying the above normalization function to all columns except the first 2
#so lapply is a function in r where can specify the function to apply and the columns on which we have to apply the same 
can_n_L <- lapply(cancer[, 3:32], normalize)
#converting the data to a data frame[Since wbcd_n_L consist of only data from #3- 32 columns]
can_n <- data.frame(can_n_L)

can_n[1:3, 1:4]

```

```{r}
rownames(can_n) <- cancer$id
```


```{r}
#Isolate the class labels and name them accordingly
BM_class <- cancer[, 2]
#names(BM_class)-> this would give null because there are no labels yet because #there #are no attributes 
names(BM_class) <- cancer$id

BM_class[1:3]
#so now each label comes under an attribute which is actually the row name/id
#imagine a single row but 569 of attributes
```

### **Creating training and test (validation) datasets**
*Surya*:
```{r}

nrow(cancer)

rand_permute <- sample(x = 1:569, size = 569)

rand_permute[1:5]

# save(rand_permute, file='rand_permute.RData')

```

```{r}

#load("rand_permute.RData")

```


```{r}
#randomly permute the rows of data
all_id_random = cancer[rand_permute, "id"]

# Select the first third of these for validation

569/3

```


```{r}
#Get the first 1/3 ids of the data and keep it for validation
validate_id <- as.character(all_id_random[1:189])
#Get the next 2/3 ids of the data and keep it for training
training_id <- as.character(all_id_random[190:569])

```

*Surya*: Subset the data by taking the data of the respective ids
```{r}

can_train <- can_n[training_id, ]

can_val <- can_n[validate_id, ]

BM_class_train <- BM_class[training_id]

BM_class_val <- BM_class[validate_id]

table(BM_class_train)

```

```{r}

table(BM_class_val)

```

### **Executing knn**
*Surya*:

```{r}

library(class)

`?`(knn)

```

```{r}

sqrt(nrow(can_train))

```

```{r}

k = 19

```


```{r}
#Fitting the model and validating against test set
knn_predict = knn(can_train, can_val, BM_class_train, k = 19)

knn_predict[1:3]

```

```{r}
#Check the confusion matrix for true positives and true negatives
table(knn_predict, BM_class_val)

```


```{r}

prop.table(table(knn_predict, BM_class_val))

```
It really depends on the randomly selectd data for testing and validating

### **Testing different values of k**

**Aylin**: The knn numerical value are given random variables to predict the outcome of the train set. The first is considered the best.

```{r}

knn_predict_3 = knn(can_train, can_val, BM_class_train, k = 3)

knn_predict_7 = knn(can_train, can_val, BM_class_train, k = 7)

knn_predict_11 = knn(can_train, can_val, BM_class_train, k = 11)

knn_predict_31 = knn(can_train, can_val, BM_class_train, k = 31)

```


```{r}

table(knn_predict_3, BM_class_val)

table(knn_predict_7, BM_class_val)

table(knn_predict_11, BM_class_val)

table(knn_predict_31, BM_class_val)

```

###**Study significance of the variables**

**Aylin**: Below the names of the data set are listed. A model is binomial regression model is created. The original name of the model was lm_1 but I renamed it g1 to make it easier. "can" is also substituted for "wbcd". The names of the model is created in the second code after the g1 model is created. Then the F statitic is created. 

```{r}

names(can_train)

```


```{r}

g1 = lm(radius_mean ~ BM_class_train, data = can_train)
summary(g1)

names(summary(g1))

summary(g1)$fstatistic

# The significance measure we want:
summary(g1)$fstatistic[1]

```

**Aylin**: The first chunk of code, a vector is created in order to keep all the outputs together. The next code the variables are run through to try to get a linear fit and have the F- statistic stored. The first code also asks NA to be repeated 30 times. The first three variables in the first row has the f statistic value. 

```{r}

exp_var_fstat <- as.numeric(rep(NA, times = 30))

names(exp_var_fstat) <- names(can_train)

exp_var_fstat["radius_mean"] <- summary(lm(radius_mean ~ BM_class_train, data = can_train))$fstatistic[1]

exp_var_fstat["texture_mean"] <- summary(lm(texture_mean ~ BM_class_train, data = can_train))$fstatistic[1]

exp_var_fstat["perimeter_mean"] <- summary(lm(perimeter_mean ~ BM_class_train, data = can_train))$fstatistic[1]

exp_var_fstat

```

#####**Looping over variable names**

**Aylin**: The last step is repeated again to create a vector in order to hold the siginficance measures. The code commented out produces an error since there is no variable with the name form exp_vars[j]. The named variabe needs to be stored in the variable named "slot", so a variable is created and a formula is created in order for this to happen in the next code snippet below the code with the error.  Again the variable exp_var_fstat is called and a table is outputted with the variables in the data set and the f statistic.

```{r}

exp_vars = names(can_train)

exp_var_fstat = as.numeric(rep(NA, times = 30))

names(exp_var_fstat) = exp_vars

# Code snippet commented out creates an error.

#for (j in 1:length(exp_vars)) {
 #    exp_var_fstat[exp_vars[j]] = summary(lm(exp_vars[j] ~ BM_class_train, data = can_train))$fstatistic[1]
#  }

for (j in 1:length(exp_vars)) {
   
   exp_var_fstat[exp_vars[j]] = 
     
      summary(lm(as.formula(paste(exp_vars[j], " ~ BM_class_train")), data = can_train))$fstatistic[1]

   }

exp_var_fstat

```

**Aylin**: The function lapply or sapply is used to avoid initializing the variables. This gets all stored in a second variable "exp_var_fstat2".

```{r}

exp_var_fstat2 = sapply(exp_vars, function(x) {
  
    summary(lm(as.formula(paste(x, " ~ BM_class_train")), data = can_train))$fstatistic[1]
  
})

exp_var_fstat2

```

```{r}

names(exp_var_fstat2) = exp_vars

```


#####**plyr version of the fit**

**Aylin**: The data is now combined together by creating a list of data.frames with one category for each variable. The BM class variable is packaged into the data.frames so all the variables are all in one location. When you get the output you will see numerical values with four categories:sample, variable, value, and the variable's class.

```{r}

can_L = lapply(exp_vars, function(x) {

      df = data.frame(sample = rownames(can_train), variable = x, value = can_train[, 
        x], class = BM_class_train)
    df
})

head(can_L[[1]])

```

```{r}

head(can_L[[5]])

```

```{r}

names(can_L) = exp_vars

```

**Aylin**: The function laply in the plyr library. The function sapply can also be the same since they are basically the same function. There are three different types of mean( radius_mean, texture_mean, perimeter_mean) along with the f statistic values created. 

```{r}

library(plyr)

var_sig_fstats = laply(can_L, function(df) {
    fit = lm(value ~ class, data = df)
    f = summary(fit)$fstatistic[1]
    f
})

names(var_sig_fstats) = names(can_L)

var_sig_fstats[1:3]


```

####**Conclusions about significance of the variables**

**Aylin**: The first code snippet is asking for the data for variables ordered 1 to 5 which is points_worst, perimeter_worst, points_mean, radius_worst, and area_worst. It then prints out for each of these variables the significant f stats for each.The same goes for the second code snippet except for data variables ordered 25 to 30. Below, the last code snippet, the variables in the training set named data.frame are reordered by significance in order to prepare to do the kNN.

```{r}

most_sig_stats = sort(var_sig_fstats, decreasing = T)

most_sig_stats[1:5]

most_sig_stats[25:30]

can_train_ord = can_train[, names(most_sig_stats)]

```


### **Monte Carlo Cross-Validation**

####**Selection of the family of training sets**

**Aylin**: The data below is subsetted. The first code takes the length of the training set, the second takes the length of the training set and multiplies it by 2/3, the third takes the length of the training set and subtracts it from 253. The value 253 is the size of the new training set. The training data gets loaded and is named training_family_L.Data.

```{r}

length(training_id)

(2/3) * length(training_id)

length(training_id) - 253


# Use 253 as the training set size.

training_family_L = lapply(1:1000, function(j) {
    perm = sample(1:380, size = 380, replace = F)
    shuffle = training_id[perm]
    trn = shuffle[1:253]
    trn
})

# save(training_family_L, file='training_family_L.RData')

#load("training_family_L.RData")

validation_family_L = lapply(training_family_L, function(x) setdiff(training_id, x))

```

####**Finding an optimal set of variables and optimal k**

**Aylin**: The code below calculates the distributions of errors over the 1000 training-validation pairs for different subsets of the variables. The square root of the reference set size is taken in order to test options for k. The value will vary from 3 to 19 and the last code, for each training - validation subset, number of variables, and k, the error of the kNN prediction in the validation set is calculated.

```{r}

N = seq(from = 3, to = 29, by = 2)

sqrt(length(training_family_L[[1]]))

K = seq(from = 3, to = 19, by = 2)

1000 * length(N) * length(K)

```


####**Execution of the test with loops**

**Aylin**: The data frame will be initialized to store 126,000 entries. A new function for the core kNN error is created and stored in the knn_test, n = 5 and k = 7.

```{r}

paramter_errors_df = data.frame(mc_index = as.integer(rep(NA, times = 126000)), 
    var_num = as.integer(rep(NA, times = 126000)), k = as.integer(rep(NA, times = 126000)), 
    error = as.numeric(rep(NA, times = 126000)))

knn_test = knn(train = can_train_ord[training_family_L[[1]], 1:5], test = can_train_ord[validation_family_L[[1]], 
    1:5], cl = BM_class_train[training_family_L[[1]]], k = 7)

knn_test[1:3]

tbl_test = table(knn_test, BM_class_train[validation_family_L[[1]]])

tbl_test

err_rate = (tbl_test[1, 2] + tbl_test[2,1])/length(validation_family_L[[1]])

err_rate

```


**Aylin**: Another function is created to run the code snippet and return back the error rate along with other parameters. Then below after "sample" commented out, a nested for loop is created.

```{r}

# j = index, n = length of range of variables, k=k

core_knn = function(j, n, k) {
    knn_predict = knn(train = can_train_ord[training_family_L[[j]], 1:n], 
        test = can_train_ord[validation_family_L[[j]], 1:n], cl = BM_class_train[training_family_L[[j]]], k = k)
    
    tbl = table(knn_predict, BM_class_train[validation_family_L[[j]]])
    err = (tbl[1, 2] + tbl[2, 1])/length(validation_family_L[[j]])
    err
}

# sample

core_knn(1, 5, 7)

iter = 1

str_time = Sys.time()

for (j in 1:1000) {
    for (n in 1:length(N)) {
        for (m in 1:length(K)) {
            err = core_knn(j, N[n], K[m])
            paramter_errors_df[iter, ] <- c(j, N[n], K[m], err)
            iter = iter + 1
        }
    }
}

time_lapsed_for = Sys.time() - str_time

save(paramter_errors_df, time_lapsed_for, file = "for_loop_paramter_errors.RData")

load("for_loop_paramter_errors.RData")
time_lapsed_for

```


####**Execution with plyr**

**Aylin**: A data frame of all possible parameter combinations is created.Then they will be nested inside the several **ply functions. Below is just a trst using 20 choices of parameters and then you do a full run.

```{r}

param_df1 = merge(data.frame(mc_index = 1:1000), data.frame(var_num = N))

param_df = merge(param_df1, data.frame(k = K))

str(param_df)

knn_err_est_df_test_test = ddply(param_df[1:20, ], .(mc_index, var_num, k), function(df) {
   
   err = core_knn(df$mc_index[1], df$var_num[1], df$k[1])
   
     err
})

head(knn_err_est_df_test_test)

```


```{r}

str_time = Sys.time()

knn_err_est_df_test_test = ddply(param_df, .(mc_index, var_num, k), function(df) {
    err = core_knn(df$mc_index[1], df$var_num[1], df$k[1])
   
    err 
}) 

time_lapsed = Sys.time() - str_time

save(knn_err_est_df_test_test, time_lapsed, file = "knn_err_est_df_test_test")

load("knn_err_est_df_test_test")

time_lapsed

head(knn_err_est_df_test_test)

names(knn_err_est_df_test_test)[4] = "error"

```


### **Getting summary performance statistics**

**Aylin**: The mean is taken for each of the parameters.Then the mean error is taken for the parameters.

```{r}

mean_ex_df = subset(knn_err_est_df_test_test, var_num == 5 & k == 7)

head(mean_ex_df)

mean(mean_ex_df$error)

mean_errs_df = ddply(knn_err_est_df_test_test, .(var_num, k), function(df) 
mean(df$error))

head(mean_errs_df)

names(mean_errs_df)[3] = "mean_error"

```


### **Survey of parameter performance**

**Aylin**: The performance gets visualized using the library ggplot. 

```{r message=FALSE, warning=FALSE}

library(ggplot2)

ggplot(data = mean_errs_df, aes(x = var_num, y = k, color = mean_error)) + geom_point(size = 10) +     theme_bw()

ggplot(data = subset(mean_errs_df, var_num >= 15), aes(x = var_num, y = k, color = mean_error)) + 
    geom_point(size = 10) + theme_bw()

```

**Aylin**: Variables with a low k works the best.

```{r}

subset(mean_errs_df, var_num == 17)

subset(mean_errs_df, var_num == 19)

subset(mean_errs_df, var_num == 21)

subset(mean_errs_df, var_num == 25)

mean_errs_df[which.min(mean_errs_df$mean_error), ]

names(can_train_ord)

```


### **Validation of the final test**

*VIRAJ*
```{r}

bcd_val_ord = can_val[, names(can_train_ord)] #Here the 189 observations are taken from both the tables out of the 30 variables 

bm_val_pred <- knn(train = can_train_ord[, 1:27], can_val[, 1:27], BM_class_train, 
    k = 3) #training both these variables with k=3 for the class train table 

tbl_bm_val <- table(bm_val_pred, BM_class_val) # pred table contains the b or m predicated values, class val table contains the dataset values whether b or m 
tbl_bm_val

(val_error <- tbl_bm_val[1, 2] + tbl_bm_val[2, 1])/length(BM_class_val) #putting these values from tbl_bm_val i.e. 34 and 3 by the length of BM_class_val to get the error in the prediction

```


